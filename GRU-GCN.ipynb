{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce593060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import related libraries\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl import function as fn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a602dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAGConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,    \n",
    "                 out_feats,   \n",
    "                 k=2,         \n",
    "                 bias=True,   \n",
    "                 activation=None): \n",
    "\n",
    "        super(TAGConv, self).__init__()   \n",
    "      \n",
    "        self._in_feats = in_feats  \n",
    "        self._out_feats = out_feats \n",
    "        self._k = k\n",
    "        self._activation = activation\n",
    "\n",
    "        self.lin = nn.Linear(in_feats * (self._k + 1), out_feats, bias=bias)  \n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "\n",
    "        self.reset_parameters() \n",
    "\n",
    "\n",
    "    def reset_parameters(self):      \n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')     \n",
    "        nn.init.xavier_normal_(self.lin.weight, gain=gain)   \n",
    "\n",
    "        \n",
    "    def forward(self, graph, feat):    \n",
    "        graph = graph.local_var()           \n",
    "        \n",
    "        norm = th.pow(graph.in_degrees().float().clamp(min=1), -0.5) \n",
    "    \n",
    "        \n",
    "        shp = norm.shape + (1,) * (feat.dim() - 1)    \n",
    "        \n",
    "        norm = th.reshape(norm, shp).to(feat.device)  \n",
    "\n",
    "        #D-1/2 A D -1/2 X\n",
    "        fstack = [feat]      \n",
    "        for _ in range(self._k):\n",
    "\n",
    "            rst = fstack[-1] * norm    \n",
    "            \n",
    "            graph.ndata['h'] = rst     \n",
    "\n",
    "            graph.update_all(fn.copy_src(src='h', out='m'),   \n",
    "                             fn.sum(msg='m', out='h'))     \n",
    "           \n",
    "            \n",
    "            rst = graph.ndata['h']     \n",
    "            rst = rst * norm           \n",
    "            fstack.append(rst)         \n",
    "\n",
    "        rst = self.lin(th.cat(fstack, dim=-1))    \n",
    "\n",
    "        if self._activation is not None:\n",
    "            rst = self._activation(rst)         \n",
    "\n",
    "        return rst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cc229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "   \n",
    "    def __init__(self,input_dim,hidden_size,num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "       \n",
    "        self.GRU = nn.GRU(input_size=input_dim, hidden_size=hidden_size, num_layers=1)\n",
    "        \n",
    "       \n",
    "        self.gcn1 = TAGConv(12,hidden_size,k=1)\n",
    "\n",
    "        \n",
    "        self.gcn2 = TAGConv(hidden_size,num_classes,k=1)\n",
    "  \n",
    "\n",
    "    def forward(self, graph, feature):\n",
    "       \n",
    "        h = self.GRU(feature)\n",
    "\n",
    "        h = F.relu(self.gcn1(graph,h[0])) #Hidden layer operation, absorbing information from neighboring nodes in the first layer\n",
    "\n",
    "        h = self.gcn2(graph, h)           #Fully connected layer operation, absorbing information from neighboring nodes in the second layer\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10a39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input adjacency matrix\n",
    "A_city=pd.read_excel(r'D:\\0_Shay\\Practise\\jupyter\\CSJ\\LJJZ.xlsx',index_col=[0])\n",
    "A_city.fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4d2de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\anaconda\\envs\\torch\\lib\\site-packages\\dgl\\base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n",
      "D:\\anaconda\\anaconda\\envs\\torch\\lib\\site-packages\\dgl\\base.py:45: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=307, num_edges=1969,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in A_city.columns:\n",
    "    for j in A_city.index:\n",
    "        if i==j:\n",
    "            A_city[i][j]=1\n",
    "A_city_array=np.array(A_city)\n",
    "G1 = dgl.DGLGraph()\n",
    "\n",
    "for i in range(len(A_city_array)):\n",
    "    for j in range(len(A_city_array)):\n",
    "        if A_city_array[i][j]==1:\n",
    "            G1.add_edge(i, j)\n",
    "        else:\n",
    "            pass\n",
    "G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a1c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input feature matrix\n",
    "df = pd.read_excel(r'D:\\0_Shay\\Practise\\jupyter\\CSJ\\GYH62.xlsx',index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb4330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lab(dataset, start, end, experience, future):   \n",
    "    data = []\n",
    "    labels = []\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    real_start = start + experience     \n",
    "\n",
    "    for i in range(real_start, end):    \n",
    "        data.append(dataset.iloc[i-experience:i])    \n",
    "        labels.append(dataset.iloc[i:i+future])    #i:i+future\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        data_tensor = th.Tensor(np.array(data[j].T))\n",
    "        data_list.append(data_tensor)\n",
    "    \n",
    "    for k in range(len(labels)):\n",
    "        labels_tensor = th.Tensor(np.array(labels[k].T))\n",
    "        labels_list.append(labels_tensor)\n",
    "    \n",
    "    return th.stack(data_list), th.stack(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc7d9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 307, 6]),\n",
       " torch.Size([24, 307, 1]),\n",
       " torch.Size([4, 307, 6]),\n",
       " torch.Size([4, 307, 6]),\n",
       " torch.Size([4, 307, 1]),\n",
       " torch.Size([4, 307, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Divide the dataset according to 6:2:2\n",
    "train_x,train_y = gen_lab(df,0,30,6,1)\n",
    "valid_x,valid_y = gen_lab(df,30,40,6,1)\n",
    "test_x,test_y = gen_lab(df,40,50,6,1)\n",
    "train_x.shape,train_y.shape,valid_x.shape,test_x.shape,valid_y.shape,test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e6ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_trainer(network,graph,input_data,label_data,training_times,\n",
    "                optimizer,criterion,loss_list,dur_list):\n",
    "   \n",
    "    #loss_list = loss_list\n",
    "    #network = network\n",
    "\n",
    "    for epoch in range(training_times):\n",
    "        t0 = time.time()\n",
    "        network.train()\n",
    "        out = network(graph,input_data)          \n",
    "        \n",
    "        #criterion = criterion\n",
    "        loss = criterion(out,label_data)\n",
    "\n",
    "        \n",
    "        #optimizer = optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        dur_list.append(time.time() - t0)\n",
    "        \n",
    "\n",
    "        \n",
    "        if (epoch+1) % 70 == 0: #epoch=70\n",
    "            #acc = evaluate(net, g, features, labels, test_mask)\n",
    "            print(\"Epoch {:04d} | MAE_Test_Loss {:.4f}\".format(epoch+1, loss.item())) \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a20b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch1: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0070 | MAE_Test_Loss 0.0254\n",
      "Batch2: Epoch 0070 | MAE_Test_Loss 0.0151\n",
      "Batch3: Epoch 0070 | MAE_Test_Loss 0.0056\n",
      "Batch4: Epoch 0070 | MAE_Test_Loss 0.0056\n",
      "Batch5: Epoch 0070 | MAE_Test_Loss 0.0055\n",
      "Batch6: Epoch 0070 | MAE_Test_Loss 0.0053\n",
      "Batch7: Epoch 0070 | MAE_Test_Loss 0.0053\n",
      "Batch8: Epoch 0070 | MAE_Test_Loss 0.0058\n",
      "Batch9: Epoch 0070 | MAE_Test_Loss 0.0159\n",
      "Batch10: Epoch 0070 | MAE_Test_Loss 0.0083\n",
      "Batch11: Epoch 0070 | MAE_Test_Loss 0.0078\n",
      "Batch12: Epoch 0070 | MAE_Test_Loss 0.0093\n",
      "Batch13: Epoch 0070 | MAE_Test_Loss 0.0099\n",
      "Batch14: Epoch 0070 | MAE_Test_Loss 0.0092\n",
      "Batch15: Epoch 0070 | MAE_Test_Loss 0.0064\n",
      "Batch16: Epoch 0070 | MAE_Test_Loss 0.0074\n",
      "Batch17: Epoch 0070 | MAE_Test_Loss 0.0069\n",
      "Batch18: Epoch 0070 | MAE_Test_Loss 0.0064\n",
      "Batch19: Epoch 0070 | MAE_Test_Loss 0.0063\n",
      "Batch20: Epoch 0070 | MAE_Test_Loss 0.0068\n",
      "Batch21: Epoch 0070 | MAE_Test_Loss 0.0129\n",
      "Batch22: Epoch 0070 | MAE_Test_Loss 0.0084\n",
      "Batch23: Epoch 0070 | MAE_Test_Loss 0.0079\n",
      "Batch24: Epoch 0070 | MAE_Test_Loss 0.0069\n"
     ]
    }
   ],
   "source": [
    "mymodel=GCN(6,12,1);GPGCN_Loss_list=[];GPGCN_Loss_list.clear();GPGCN_MAE_Dur_list=[]\n",
    "for i in range(len(train_x)):\n",
    "    print('Batch{:d}: '.format(i+1),end='')\n",
    "    gcn_trainer(mymodel,G1,train_x[i],train_y[i],70,th.optim.Adam(mymodel.parameters(),lr=1e-4),nn.L1Loss(),GPGCN_Loss_list,GPGCN_MAE_Dur_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b709ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_valid(network,test_input,test_label,criterion,loss_list,out_list):\n",
    "\n",
    "    for i in range(len(test_input)):\n",
    "        loss = criterion(network(G1,test_input[i]),test_label[i])\n",
    "        loss_list.append(loss)\n",
    "        out = network(G1,test_input[i])\n",
    "        out_list.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ede46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPGCN_valid_loss_list=[];GPGCN_valid_loss_list.clear();GPGCN_valid_out_list=[];GPGCN_valid_out_list.clear();\n",
    "gcn_valid(mymodel,valid_x,valid_y,nn.L1Loss(),GPGCN_valid_loss_list,GPGCN_valid_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2309a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_tester(network,test_input,test_label,criterion,loss_list,out_list):\n",
    "    for i in range(len(test_input)):\n",
    "        loss = criterion(network(G1,test_input[i]),test_label[i])\n",
    "        loss_list.append(loss)\n",
    "        out = network(G1,test_input[i])\n",
    "        out_list.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0163a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPGCN_test_loss_list=[];GPGCN_test_loss_list.clear();GPGCN_test_out_list=[];GPGCN_test_out_list.clear()\n",
    "gcn_tester(mymodel,test_x,test_y,nn.L1Loss(),GPGCN_test_loss_list,GPGCN_test_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc479a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The trained Yangtze River Delta model in the paper\n",
    "CSJmodel = th.load(r'D:\\0_Shay\\Practise\\jupyter\\trained\\aftertained\\CSJ.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29a076a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPGCN_test_loss_list=[];GPGCN_test_loss_list.clear();GPGCN_test_out_list=[];GPGCN_test_out_list.clear()\n",
    "gcn_tester(CSJmodel,test_x,test_y,nn.L1Loss(),GPGCN_test_loss_list,GPGCN_test_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae5ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following is a multi-step prediction under recursive strategy\n",
    "def gen_labpre(dataset, start, end, experience, future):   \n",
    "    data = []\n",
    "    labels = []\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    real_start = start + experience     \n",
    "\n",
    "    for i in range(real_start, end+1):    \n",
    "        data.append(dataset.iloc[i-experience:i])    \n",
    "        labels.append(dataset.iloc[i:i+future])    #i:i+future\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        data_tensor = th.Tensor(np.array(data[j].T))\n",
    "        data_list.append(data_tensor)\n",
    "    \n",
    "    for k in range(len(labels)):\n",
    "        labels_tensor = th.Tensor(np.array(labels[k].T))\n",
    "        labels_list.append(labels_tensor)\n",
    "    \n",
    "    return th.stack(data_list), th.stack(labels_list)\n",
    "\n",
    "testlist = pd.read_excel(r'D:\\0_Shay\\Practise\\jupyter\\trained\\prediction4.xlsx',index_col=[0])\n",
    "\n",
    "for i in range(4):\n",
    "    test_xpre,test_ypre = gen_labpre(testlist,0,6+i,6,0)\n",
    "    predict1=CSJmodel(G1,test_xpre[-1])\n",
    "    predict1\n",
    "    out_list = predict1.tolist()\n",
    "    out_list = np.transpose(out_list).tolist()\n",
    "    testlist=testlist.append(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72518bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
